/// Conversation example demonstrating more advanced features.
///
/// This example shows how to:
/// - Use system messages
/// - Configure temperature and other settings
/// - Create a multi-turn conversation simulation
///
/// Run with:
/// ```bash
/// export OPENAI_API_KEY="your-api-key"
/// cargo run --example conversation
/// ```
use ai_sdk_core::GenerateTextBuilder;
use ai_sdk_core::prompt::Prompt;
use ai_sdk_openai_compatible::OpenAICompatibleClient;
use std::env;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ¤– AI SDK Rust - Conversation Example\n");

    // Get API key from environment
    let api_key = env::var("OPENAI_API_KEY").map_err(
        |_| "OPENAI_API_KEY environment variable not set. Please set it with your API key.",
    )?;

    // Create OpenAI provider using the client builder
    let provider = OpenAICompatibleClient::new()
        .base_url("https://openrouter.ai/api/v1")
        .api_key(api_key)
        .build();

    let model = provider.chat_model("gpt-4o-mini");

    println!("âœ“ Using model: {} ({})", model.model_id(), model.provider());
    println!();

    // Example 1: With system message
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Example 1: Using System Messages");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let prompt = Prompt::text("Explain quantum computing")
        .with_system("You are a helpful teacher who explains complex topics simply.");

    println!("ğŸ“¤ System: You are a helpful teacher who explains complex topics simply.");
    println!("ğŸ“¤ User: Explain quantum computing\n");

    println!("â³ Generating response...\n");
    let result = GenerateTextBuilder::new(model.clone(), prompt)
        .temperature(0.7)
        .max_output_tokens(150)
        .execute()
        .await?;

    println!("ğŸ“ Response:");
    for content in &result.content {
        println!("{:?}\n", content);
    }
    println!(
        "ğŸ“Š Tokens used: {} in, {} out\n",
        result.usage.input_tokens, result.usage.output_tokens
    );

    // Example 2: Different temperature settings
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Example 2: Temperature Comparison");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let creative_question = "Write a creative opening line for a sci-fi novel.";

    // Low temperature (more focused)
    println!("ğŸ“¤ Prompt: {}", creative_question);
    println!("ğŸŒ¡ï¸  Temperature: 0.2 (focused)\n");

    let focused_prompt = Prompt::text(creative_question);

    let focused_result = GenerateTextBuilder::new(model.clone(), focused_prompt)
        .temperature(0.2)
        .max_output_tokens(100)
        .execute()
        .await?;

    println!("ğŸ“ Focused Response:");
    for content in &focused_result.content {
        println!("{:?}\n", content);
    }

    // High temperature (more creative)
    println!("ğŸŒ¡ï¸  Temperature: 1.2 (creative)\n");

    let creative_prompt = Prompt::text(creative_question);

    let creative_result = GenerateTextBuilder::new(model, creative_prompt)
        .temperature(1.2)
        .max_output_tokens(100)
        .execute()
        .await?;

    println!("ğŸ“ Creative Response:");
    for content in &creative_result.content {
        println!("{:?}\n", content);
    }

    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("\nâœ… Example completed successfully!");
    println!("ğŸ’¡ Notice how temperature affects the creativity of responses.");

    Ok(())
}
