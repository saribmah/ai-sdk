//! Storage integration with StreamText example.
//!
//! This example demonstrates how to automatically store streamed conversations
//! when using StreamText with the filesystem storage provider.
//!
//! Run with:
//! ```bash
//! export OPENAI_API_KEY="your-api-key"
//! cargo run --example storage_with_stream
//! ```

use ai_sdk_core::StreamText;
use ai_sdk_core::prompt::Prompt;
use ai_sdk_openai_compatible::OpenAICompatibleClient;
use ai_sdk_storage::StorageProvider;
use ai_sdk_storage_filesystem::FilesystemStorageProvider;
use futures_util::StreamExt;
use std::env;
use std::sync::Arc;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ðŸ¤– AI SDK - Storage with StreamText Example\n");

    // Get API key from environment
    let api_key = env::var("OPENAI_API_KEY").map_err(
        |_| "OPENAI_API_KEY environment variable not set. Please set it with your API key.",
    )?;

    println!("âœ“ API key loaded from environment\n");

    // Create OpenAI provider
    let provider = OpenAICompatibleClient::new()
        .base_url("https://openrouter.ai/api/v1")
        .api_key(api_key)
        .build();

    let model = provider.chat_model("gpt-4o-mini");
    println!("âœ“ Model: {}\n", model.model_id());

    // Create filesystem storage provider
    let storage_path = std::env::temp_dir().join("ai-sdk-storage-stream-example");
    println!("ðŸ“ Storage path: {}", storage_path.display());

    let storage = Arc::new(FilesystemStorageProvider::new(&storage_path)?);
    storage.initialize().await?;
    println!("âœ“ Storage initialized\n");

    // Create a session ID for this conversation
    let session_id = format!("stream-session-{}", chrono::Utc::now().timestamp());
    println!("ðŸ’¬ Session ID: {}\n", session_id);

    // Stream a response with storage
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Streaming Question: Tell me a short story about Rust");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let prompt = Prompt::text("Tell me a very short 2-sentence story about a Rust programmer.");

    let result = StreamText::new(model.clone(), prompt)
        .with_storage(storage.clone())
        .with_session_id(session_id.clone())
        .temperature(0.8)
        .execute()
        .await?;

    println!("ðŸ¤– Response (streaming):\n");

    let mut stream = result.text_stream();
    let mut full_text = String::new();

    while let Some(text) = stream.next().await {
        print!("{}", text);
        full_text.push_str(&text);
        // Flush stdout to see the streaming effect
        use std::io::Write;
        std::io::stdout().flush().unwrap();
    }

    println!("\n\nâœ“ Stream completed");
    println!("âœ“ Messages automatically stored to session\n");

    // Retrieve and display conversation history
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Retrieving Conversation History");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let conversation_storage = storage.conversation_storage();
    let messages = conversation_storage.get_messages(&session_id, None).await?;

    println!("ðŸ“œ Stored {} messages in this session:\n", messages.len());

    for (i, msg) in messages.iter().enumerate() {
        let role = match msg.role {
            ai_sdk_storage::MessageRole::User => "ðŸ‘¤ User",
            ai_sdk_storage::MessageRole::Assistant => "ðŸ¤– Assistant",
            ai_sdk_storage::MessageRole::System => "âš™ï¸  System",
            ai_sdk_storage::MessageRole::Tool => "ðŸ”§ Tool",
        };

        println!("{}. {} ({})", i + 1, role, msg.id);
        if let Some(text) = msg.content.get("text").and_then(|v| v.as_str()) {
            println!("   {}\n", text);
        }

        // Show metadata for assistant messages
        if matches!(msg.role, ai_sdk_storage::MessageRole::Assistant)
            && let Some(usage) = &msg.metadata.usage
        {
            println!("   ðŸ“Š Token usage:");
            if let Some(prompt_tokens) = usage.prompt_tokens {
                println!("      Prompt: {}", prompt_tokens);
            }
            if let Some(completion_tokens) = usage.completion_tokens {
                println!("      Completion: {}", completion_tokens);
            }
            if let Some(total_tokens) = usage.total_tokens {
                println!("      Total: {}", total_tokens);
            }
            println!();
        }
    }

    // Stream another response to the same session
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Streaming Follow-up: What makes Rust special?");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let prompt2 = Prompt::text("What makes Rust special? Answer in one sentence.");

    let result2 = StreamText::new(model.clone(), prompt2)
        .with_storage(storage.clone())
        .with_session_id(session_id.clone())
        .temperature(0.7)
        .execute()
        .await?;

    println!("ðŸ¤– Response (streaming):\n");

    let mut stream2 = result2.text_stream();

    while let Some(text) = stream2.next().await {
        print!("{}", text);
        use std::io::Write;
        std::io::stdout().flush().unwrap();
    }

    println!("\n\nâœ“ Second stream completed\n");

    // Show updated conversation history
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Updated Conversation History");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let messages = conversation_storage.get_messages(&session_id, None).await?;

    println!("ðŸ“œ Now {} messages in session:\n", messages.len());
    for (i, msg) in messages.iter().enumerate() {
        let role = match msg.role {
            ai_sdk_storage::MessageRole::User => "ðŸ‘¤",
            ai_sdk_storage::MessageRole::Assistant => "ðŸ¤–",
            ai_sdk_storage::MessageRole::System => "âš™ï¸",
            ai_sdk_storage::MessageRole::Tool => "ðŸ”§",
        };
        println!("{}. {} {:?}", i + 1, role, msg.role);
    }
    println!();

    // Cleanup
    println!("ðŸ§¹ Cleaning up...");
    conversation_storage.delete_session(&session_id).await?;
    println!("âœ“ Session deleted\n");

    println!("âœ¨ Example completed successfully!");
    println!("\nðŸ’¡ Key takeaways:");
    println!("   â€¢ StreamText supports automatic storage just like GenerateText");
    println!("   â€¢ Messages are stored after the stream completes");
    println!("   â€¢ Full conversation context is preserved across multiple streams");
    println!("   â€¢ Token usage and metadata are automatically captured");

    Ok(())
}
