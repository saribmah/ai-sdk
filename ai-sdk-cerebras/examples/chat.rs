/// Basic chat example using ai-sdk-provider traits only.
///
/// This example demonstrates direct usage of the LanguageModel trait
/// without ai-sdk-core abstractions.
///
/// Run with:
/// ```bash
/// export CEREBRAS_API_KEY="your-api-key"
/// cargo run --example chat
/// ```
use ai_sdk_cerebras::CerebrasClient;
use ai_sdk_provider::language_model::call_options::LanguageModelCallOptions;
use ai_sdk_provider::language_model::prompt::LanguageModelMessage;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ðŸ¤– Cerebras Chat Example (Provider Traits)\n");

    // Load environment variables
    dotenvy::dotenv().ok();

    // Get API key from environment
    let api_key = std::env::var("CEREBRAS_API_KEY")
        .map_err(|_| "CEREBRAS_API_KEY environment variable not set")?;

    println!("âœ“ API key loaded from environment");

    // Create provider using builder
    let provider = CerebrasClient::new().api_key(api_key).build();

    println!("âœ“ Provider created: {}", provider.name());
    println!("âœ“ Base URL: {}\n", provider.base_url());

    // Get a language model
    let model = provider.chat_model("llama-3.3-70b");
    println!("âœ“ Model: {}", model.model_id());
    println!("âœ“ Provider: {}\n", model.provider());

    // Create a simple prompt using provider types
    let messages = vec![LanguageModelMessage::user_text(
        "What is the capital of France? Answer in one sentence.",
    )];

    println!("ðŸ“¤ Sending prompt...\n");

    // Call do_generate directly (provider trait method)
    let options = LanguageModelCallOptions::new(messages)
        .with_temperature(0.7)
        .with_max_output_tokens(100);

    let result = model.do_generate(options).await?;

    println!("âœ… Response received!\n");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ðŸ“ Content:");
    for (i, content) in result.content.iter().enumerate() {
        println!("  [{}] {:?}", i + 1, content);
    }
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    println!("\nðŸ“Š Metadata:");
    println!("  â€¢ Finish reason: {:?}", result.finish_reason);
    println!("  â€¢ Input tokens: {}", result.usage.input_tokens);
    println!("  â€¢ Output tokens: {}", result.usage.output_tokens);
    println!("  â€¢ Total tokens: {}", result.usage.total_tokens);

    if result.usage.reasoning_tokens > 0 {
        println!("  â€¢ Reasoning tokens: {}", result.usage.reasoning_tokens);
    }

    println!("\nâœ… Example completed successfully!");

    Ok(())
}
